--source include/have_innodb.inc

--disable_warnings
DROP TABLE if exists t1;
--enable_warnings

let $page_size = query_get_value(select @@innodb_page_size, @@innodb_page_size, 1);

# Create table.
CREATE TABLE t1 (a INT NOT NULL PRIMARY KEY AUTO_INCREMENT, b VARCHAR(256), KEY SECOND(a, b)) ENGINE=INNODB;

--echo ## Test-1 defragment an empty table
alter table t1 defragment;

--echo ## Test-2 defragment a single page table
INSERT INTO t1 VALUES (1, REPEAT('A', 256));
INSERT INTO t1 VALUES (2, REPEAT('A', 256));
INSERT INTO t1 VALUES (3, REPEAT('A', 256));
INSERT INTO t1 (b) SELECT b from t1;
INSERT INTO t1 (b) SELECT b from t1;

alter table t1 defragment;

--echo ## Test-3 defragment (somewhat) in parallel with delete queries

let $delete_size = 120;
delimiter //;
create procedure defragment()
begin
  set @i = 0;
  repeat
    set @i = @i + 1;
    alter table t1 defragment;
    select sleep(0.5);
  until @i = 3 end repeat;
end //
delimiter ;//


# Populate table.
--disable_query_log
INSERT INTO t1 (b) SELECT b from t1;
INSERT INTO t1 (b) SELECT b from t1;
INSERT INTO t1 (b) SELECT b from t1;
INSERT INTO t1 (b) SELECT b from t1;
INSERT INTO t1 (b) SELECT b from t1;
INSERT INTO t1 (b) SELECT b from t1;
INSERT INTO t1 (b) SELECT b from t1;
INSERT INTO t1 (b) SELECT b from t1;
INSERT INTO t1 (b) SELECT b from t1;
INSERT INTO t1 (b) SELECT b from t1;
--enable_query_log

select count(*) from t1;

connect (con1,localhost,root,,test,$MASTER_MYPORT,$MASTER_MYSOCK);

connection con1;
--send call defragment()

connection default;

--disable_query_log
let $size = $delete_size;
while ($size)
{
    let $j =  100 * $size;
    eval delete from t1 where a between $j - 20 and $j;
    dec $size;
}
--enable_query_log

connection con1;
--disable_result_log
--reap
--enable_result_log

connection default;
disconnect con1;

alter table t1 defragment;

--source include/restart_mysqld.inc
select count(*) from t1;

# Before defragmentation, there are 12288 records. After deletion, there are 10203 records left.
# A 16K page can hold about 57 records. We fill the page 90% full, so there should be less than
# 205 pages total after defragmentation for 16K pages. For 32K page it should be about half
# the number.

let $expected = 205;
if ($page_size == 32768) {
   let $expected = 103;
}

--let $result = query_get_value(select count(*) as result from information_schema.innodb_buffer_page where table_name like '%t1%' and index_name = 'PRIMARY' and access_time > 0, result, 1)

if ($result > $expected) {
  --echo fail, $result > $expected
}

select count(*) from t1 force index (second);

# secondary index is slightly bigger than primary index so the number of pages should be similar.
let $expected = 210;
if ($page_size == 32768) {
   let $expected = 105;
}
--replace_result $expected expected
eval select count(*) < $expected from information_schema.innodb_buffer_page where table_name like '%t1%' and index_name = 'second' and access_time > 0;

--echo ## Test-4 defragment with larger n_pages

# delete some more records
--disable_query_log
let $size = $delete_size;
while ($size)
{
    let $j = 100 * $size;
    eval delete from t1 where a between $j - 30 and $j - 20;
    dec $size;
}
--enable_query_log

SET @@global.innodb_defragment_n_pages = 3;

# This will not reduce number of pages by a lot
alter table t1 defragment;

--source include/restart_mysqld.inc

select count(*) from t1;

# We didn't create large wholes with the previous deletion, so if innodb_defragment_n_pages = 3, we won't be able to free up many pages.
let $expected = 195;
if ($page_size == 32768) {
   let $expected = 97;
}
--replace_result $expected expected
eval select count(*) > $expected from information_schema.innodb_buffer_page where table_name like '%t1%' and index_name = 'PRIMARY' and access_time > 0;

select count(*) from t1 force index (second);

# Same holds for secondary index, not many pages are released.
--replace_result $expected expected
eval select count(*) > $expected from information_schema.innodb_buffer_page where table_name like '%t1%' and index_name = 'second' and access_time > 0;

SET @@global.innodb_defragment_n_pages = 10;

alter table t1 defragment index PRIMARY;

--source include/restart_mysqld.inc

select count(*) from t1;

# This time we used innodb_defragment_n_pages = 10, so we should be able to free up some pages.
let $expected_new = 185;
if ($page_size == 32768) {
   let $expected_new = 93;
}

--let $result = query_get_value(select count(*) as result from information_schema.innodb_buffer_page where table_name like '%t1%' and index_name = 'PRIMARY' and access_time > 0, result, 1)

if ($result >= $expected_new) {
  --echo fail, $result >= $expected_new
  select * from information_schema.innodb_buffer_page where table_name like '%t1%' and index_name = 'PRIMARY' order by page_number;
}

select count(*) from t1 force index (second);

# We only defragmented the primary index so secondary index size should not change.
--replace_result $expected expected
eval select count(*) > $expected from information_schema.innodb_buffer_page where table_name like '%t1%' and index_name = 'second' and access_time > 0;

SET @@global.innodb_defragment_n_pages = 10;

alter table t1 defragment index second;
--source include/restart_mysqld.inc
select count(*) from t1 force index (second);

# After defragmenting secondary index the size should be smaller.
--replace_result $expected_new expected
eval select count(*) < $expected_new from information_schema.innodb_buffer_page where table_name like '%t1%' and index_name = 'second' and access_time > 0;

--echo ## Test-5 kill a defragment command
let $orig_frequency = `select @@global.innodb_defragment_frequency`;
# Make the defragmentation really slow.
set global innodb_defragment_frequency = 1;
--disable_query_log
let $size = $delete_size;
while ($size)
{
    let $j =  100 * $size;
    eval delete from t1 where a between $j - 40 and $j-30;
    dec $size;
}
--enable_query_log

connect(con1,localhost,root,,);
connection con1;
let $conn_id = `select connection_id()`;
send alter table t1 defragment;
connection default;
--replace_result $conn_id ID
eval kill query $conn_id;
# Because the defragment query thread wakes up every second to check whether it's killed, there
# could be a delay from kill command issued to query actually being killed. So we wait a few second
# to make sure the query should already realize it's been killed.
select sleep(3);
connection con1;
--error ER_QUERY_INTERRUPTED
reap;

connection default;
disconnect con1;
eval set global innodb_defragment_frequency = $orig_frequency;

--echo ## Test-6 defragment index already being defragmented.
alter table t1 defragment index PRIMARY async_commit;
--error ER_SP_ALREADY_EXISTS
alter table t1 defragment index PRIMARY;

call mtr.add_suppression("InnoDB: Warning: MySQL is trying to drop table ");
DROP PROCEDURE defragment;
DROP TABLE t1;
